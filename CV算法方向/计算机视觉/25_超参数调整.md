## 超参数调整

**所谓超参数，即不是通过学习算法本身学习出来的，需要作者手动调整（可优化参数）的参数**(理论上我们也可以设计一个嵌套的学习过程，一个学习算法为另一个学习算法学出最优超参数)，卷积神经网络中常见的超参数有: 优化器学习率、训练 `Epochs` 数、批次大小 `batch_size` 、输入图像尺寸大小。

一般而言，我们将训练数据分成两个不相交的子集，其中一个用于学习参数，另一个作为验证集，用于估计训练中或训练后的泛化误差，用来更新超参数。

- 用于学习参数的数据子集通常仍被称为训练集（不要和整个训练过程用到的更大的数据集搞混）。
- 用于挑选超参数的数据子集被称为验证集(`validation set`)。

通常，`80%` 的训练数据用于训练，`20%` 用于验证。因为验证集是用来 “训练” 超参数的，所以**验证集的误差通常会比训练集误差小**，验证集会低估泛化误差。完成所有超参数优化后，**需要使用测试集估计泛化误差**。

### 不同权重初始化方法

​	尝试不同的卷积核权重初始化方式。目前**常用的权重初始化方法**有 `Xavier` 和 `kaiming` 系列，pytorch 在 `torch.nn.init` 中提供了常用的初始化方法函数，默认是使用 `kaiming` 均匀分布函数: `nn.init.kaiming_uniform_()`。

​	下面是一个使用 `kaiming_normal_`（kaiming 正态分布）设置卷积层权重初始化的示例代码。

```python
import torch
import torch.nn as nn
 
# 定义一个卷积层
conv = nn.Conv2d(in_channels=3, out_channels=16, kernel_size=3, stride=1, padding=1)
 
# 使用He初始化方式设置卷积层的权重
nn.init.kaiming_normal_(conv.weight, mode="fan_out", nonlinearity="relu")
```

使用不同的卷积层权重初始化方式，会有不同的输出效果。分别使用 `xavier_normal_` 和 `xavier_normal_` 初始化权重，并使一个输入图片经过一层卷积层，其输出效果是不同的，对比图如下所示:

![image.png](https://s2.loli.net/2024/08/13/Y6NyOmGQ1jD2KLH.png)

## Batch_size 设定

​	深度学习中经常看到 epoch、 iteration 和 batchsize，这三个名字的区别如下：

- `batch size`：批大小。在深度学习中，一般采用 SGD 训练，即每次训练在训练集中取 batch_size 个样本训练；
- `iteration`：1 个 iteration 等于使用 batch_size 个样本训练一次；
- `epoch`：1 个 epoch 等于使用训练集中的全部样本训练一次；

### 选择合适大小的 batch size

`batch size` 是所有超参数中最好调的一个，也是应该最早确定下来的超参数，其设置的原则就是，**`batch size` 别太小，也别太大，取中间合适值为宜**，通常最好是 2 的 n 次方，如 16, 32, 64, 128。在常见的 setting（～100 epochs），batch size 一般不会低于 `16`。

> 设置为 2 的 n 次方的原因：计算机的 gpu 和 cpu 的 memory 都是 2 进制方式存储的，设置 2 的 n 次方可以加快计算速度。

`batch size` 太小和太大的问题:

- `batch size` 太小：每次计算的梯度不稳定，引起训练的震荡比较大，很难收敛。
- `batch size` 太大: 虽然大的 batch size 可以减少训练时间，即收敛得快，但深度学习的**优化**（training loss 降不下去）和**泛化**（generalization gap 很大）都会出问题。

​	有论文指出 `LB`（Large batch size）之所以出现 `Generalization Gap` 问题，是因为 `LB` 训练的时候更容易收敛到 `sharp minima`，而 `SB` （Small batch size）则更容易收敛到 `flat minima`，并且 `LB` 还不容易从这些 `sharp minima` 中出来，另外，作者认为关于 `batch size` 的选择是有一个阈值的，一旦超过这个阈值，模型的质量会退化，网络的准确度大幅下降。

​	后面有相关研究人员指出**大的batchsize性能下降是因为训练时间不够长，本质上并不少batchsize的问题，在同样的epochs下的参数更新变少了，因此需要更长的迭代次数。**

## 学习率和 batch size 关系

​	`batch size` 和学习率有紧密联系，我们知道深度学习模型多采用批量随机梯度下降算法进行优化，随机梯度下降算法的原理如下:
$$
w_{t+1}=w_t-η\frac{1}{n}\Sigma_{x\in\beta}\Delta l(x,w_t)
$$


​	n 是批量大小(batchsize)，η 是学习率(learning rate)。从随机梯度下降算法（SGD），可知道除了梯度本身，这两个因子直接决定了模型的权重更新，从优化本身来看它们是影响模型性能收敛最重要的参数。

​	**学习率（`learning rate`）直接影响模型的收敛状态，`batch size` 则影响模型的泛化性能**，两者又是分子分母的直接关系，相互也可影响，因此这一次来详述它们对模型性能的影响。

​	如果增加了学习率，那么batchsize最好也跟着增加，这样收敛更稳定（因为batchsize大的话，每一步更新的准确性会更好，也就可以放心的往前走了）。适当使用大的学习率，因为很多研究都表明更大的学习率有利于提高泛化能力。如果要衰减，可以先尝试其他办法，比如增加batchsize，学习率对模型的收敛影响比较大，需慎重且多次调整。



## 优化器选择

### 优化器定义

优化器（优化算法）优化的是神经元参数的取值$(w、b)$。优化过程如下：假设 θ 表示神经网络中的参数，J(θ) 表示在给定的参数取值下，训练数据集上损失函数的大小（包括正则化项），则优化过程即为寻找某一参数 θ，使得损失函数 J(θ) 最小。

在完成数据预处理、数据增强，模型构建和损失函数确定之后，深度学习任务的数学模型也就确定下来了，接下来自然就是选择一个合适的优化器(`Optimizer`)对该深度学习模型进行优化（优化器选择好后，选择合适的学习率调整策略也很重要）。

### 如何选择适合不同ml项目的优化器

选择优化器的问题在于**没有一个可以解决所有问题的单一优化器**。实际上，优化器的性能高度依赖于设置。所以，优化器选择的本质其实是: **哪种优化器最适合自身项目的特点？**

深度卷积神经网络通常采用随机梯度下降类型的优化算法进行模型训练和参数求解。最为常用且经典的优化器算法是 （基于动量的）随机梯度下降法 `SGD（stochastic gradient descent）` 和 `Adam` 法，其他常见的优化器算法有 `Nesterov` 型动量随机下降法、`Adagrad` 法、`Adadelta` 法、`RMSProp` 法。

优化器的选择虽然没有通用的准则，但是也还是有些经验可以总结的:

- `SGD` 是最常见的神经网络优化方法，收敛效果较稳定，但是收敛速度过慢。
- `Adam` 等自适应学习率算法对于稀疏数据具有优势，且且收敛速度很快，但是收敛效果不稳定（容易跳过全局最优解）。

下表 1 概述了几种优化器的优缺点，通过下表可以尝试找到与数据集特征、训练设置和项目目标相匹配的优化器。

某些优化器在具有稀疏特征的数据上表现出色，而其他优化器在将模型应用于以前未见过的数据时可能表现更好。一些优化器在大批量（`batch_size` 设置较大）下工作得很好，而而另一些优化器会在泛化不佳的情况下收敛到极小的最小值。

![image.png](https://s2.loli.net/2024/08/13/VH8IanvqZ46PTzX.png)

> 表格来源 [Which Optimizer should I use for my ML Project?](https://www.lightly.ai/post/which-optimizer-should-i-use-for-my-machine-learning-project)

网络上有种 `tricks` 是将 `SGD` 和 `Adam` 组合使用，先用 `Adam` 快速下降，再用 `SGD` 调优。但是这种策略也面临两个问题: 什么时候切换优化器和切换后的 SGD 优化器使用什么样的学习率？论文 `SWATS` [Improving Generalization Performance by Switching from Adam to SGD](https://arxiv.org/abs/1712.07628)给出了答案，感兴趣的读者可以深入阅读下 `paper`。

### PyTorch 中的优化器

以 `Pytorch` 框架为例，PyTorch 中所有的优化器(如: optim.Adadelta、optim.SGD、optim.RMSprop 等)均是 `Optimizer` 的子类，Optimizer 中也定义了一些常用的方法:

- `zero_grad()`: 将梯度清零。
- `step(closure)`: 执行一步权值更新, 其中可传入参数 closure(一个闭包)。
- `state_dict()`: 获取模型当前的参数，以一个有序字典形式返回，key 是各层参数名，value 就是参数。
- `load_state_dict(state_dict)`: 将 state_dict 中的参数加载到当前网络，常用于模型 `finetune`。
- `add_param_group(param_group)`: 给 optimizer 管理的参数组中增加一组参数，可为该组参数定制 lr, momentum, weight_decay 等，在 finetune 中常用。

优化器设置和使用的模板代码如下:

```python
# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
# 指定每一层的学习率
optim.SGD([
            {'params': model.base.parameters()},
            {'params': model.classifier.parameters(), 'lr': 1e-3}
          ], lr=1e-2, momentum=0.9
        )
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

优化器的算法原理可以参考花书第八章内容，`Pytorch` 框架优化器类的详细参数及类方法定义，请参考 pytorch 官方库文档-[torch.optim](https://pytorch.org/docs/stable/optim.html#torch.optim.Optimizer)。

## 学习率调整

以 pytorch 框架为例，pyTorch 提供了六种学习率调整方法，可分为三大类，分别是:

有序调整： 按照一定规律有序进行调整，这一类是最常用的，分别是等间隔下降(Step)，
按需设定下降间隔(`MultiStep)`，指数下降(`Exponential`)和 `CosineAnnealing`。这四种方法的调整时机都是人为可控的，也是训练时常用到的。
自适应调整: 如依据训练状况伺机调整` ReduceLROnPlateau` 方法。该法通过监测某一指标的变化情况，当该指标不再怎么变化的时候，就是调整学习率的时机，因而属于自适应的调整。
自定义调整: 自定义调整 Lambda。Lambda 方法提供的调整策略十分灵活，我们可以为不同的层设定不同的学习率调整方法，这在 fine-tune 中十分有用，我们不仅可为不同的层设定不同的学习率，还可以为其设定不同的学习率调整策略，简直不能更棒了!
常见的学习率调整方法有:

`lr_scheduler.StepLR:` 等间隔调整学习率。调整倍数为 gamma 倍，调整间隔为 step_size。
`lr_scheduler.MultiStepLR:` 按设定的间隔调整学习率。适合后期使用，通过观察 loss 曲线，手动定制学习率调整时机。
`lr_scheduler.ExponentialLR: `按指数衰减调整学习率，调整公式: $lr=lr*gamma^{epoch}$
`lr_scheduler.CosineAnnealingLR: `以余弦函数为周期，并在每个周期最大值时重新设置学习率。
`lr_scheduler.ReduceLROnPlateau: `当某指标不再变化(下降或升高)，调整学习率（非常实用的策略）。
`lr_scheduler.LambdaLR:` 为不同参数组设定不同学习率调整策略。
学习率调整方法类的详细参数及类方法定义，请参考 pytorch 官方库文档-torch.optim。

> 注意，PyTorch 1.1.0 之后版本，学习率调整策略的设定必须放在优化器设定的后面! 构建一个优化器，首先需要为它指定一个待优化的参数的可迭代对象，然后设置特定于优化器的选项，比如学习率、权重衰减策略等。

在 PyTorch 1.1.0 之前，学习率调度器应该在优化器更新之前被调用；1.1.0 以打破 BC 的方式改变了这种行为。 如果在优化器更新（调用 optimizer.step()）之前使用学习率调度器（调用 scheduler.step()），这将跳过学习率调度的第一个值。

使用指数级衰减的学习率调整策略的模板代码如下。

```python
import torchvision.models as models
import torch.optim as optim
model = models.resnet50(pretrained=False)

optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) # 构建优化器，lr 是初始学习率
scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9) # 设定学习率调整策略

for epoch in range(20):
    for input, target in dataset:
        optimizer.zero_grad()
        output = model(input)
        loss = loss_fn(output, target)
        loss.backward()
        optimizer.step()
    scheduler.step()
    print_lr(is_verbose=true) # pytorch 新版本可用，1.4及以下版本不可用
```

## 参考

[超参数设定](https://www.cnblogs.com/armcvai/p/16977413.html#%E5%AD%A6%E4%B9%A0%E7%8E%87%E5%92%8C-batch-size-%E5%85%B3%E7%B3%BB)