## 知识蒸馏

### 为什么要知识蒸馏

​	**深度学习的主要挑战在于，受限制于资源容量，深度神经模型很难部署在资源受限制的设备上**。如嵌入式设备和移动设备。因此，涌现出了大量的模型压缩和加速技术，知识蒸馏是其中的代表，可以有效的从大型的教师模型中学习到小型的学生模型。

### 模型压缩和加速技术

1. 参数剪枝和共享：该方法专注于去除模型中的不必要参数
2. 低秩分解：该方法利用矩阵分解和张量分解来识别深度神经网络的冗余参数
3. 转移紧凑型卷积滤波器:该方法通过转移或压缩卷积滤波器来去除不必要的参数
4. 知识蒸馏(KD)：该方法将知识从一个较大的深度神经网络中提取到一个较小的网络中

![image.png](https://s2.loli.net/2024/08/06/tU29w8RiL4BSXFY.png)

​	知识蒸馏主要思想是：学生模型模仿教师模型，二者相互竞争，使得学生模型可以与教师模型持平甚至卓越的表现。关键问题是如何将知识从大的教师模型转移到小的学生模型。**知识蒸馏系统由知识、蒸馏算法和师生架构三个关键部分组成。**如上图所示

### 知识蒸馏中不同的知识类型

​	在知识蒸馏中，知识类型、蒸馏策略和师生架构对学生模型的学习起着至关重要的作用。原始知识蒸馏使用大深度模型的对数作为教师知识（Hinton 2015），中间层的激活、神经元或特征也可以作为指导学生模型学习的知识,不同的激活、神经元或成对样本之间的关系包含了教师模型所学习到的丰富信息。

![image.png](https://s2.loli.net/2024/08/06/z4xp7u9aeGYvHoh.png)

​	目前主流的知识来源为`Relation-Based Knowledge`、`Feature-Based Knowledge`以及`Response-Base Knowledge`。

#### **Response-Base Knowledge**

​	基于响应的知识，是Hinton最先提出来的知识手段，通常是指教师模型的最后一输出层的神经反应。其主要思想是让学生模型直接模仿教师模式的最终预测，假定对数向量Z为全连接层的最后输出，response-based knowledge蒸馏形式可以被描述为：
$$
L_{ResD}(z_t,z_s)=L_R(z_t,z_s)
$$
$L_R$表示散度损失，如下图所示为Response-Based 知识蒸馏模型图。

![image.png](https://s2.loli.net/2024/08/06/GDQhbtrIgn1AUJS.png)

​	

​	最受欢迎的Response-Based 知识蒸馏方法为软标签，具体来说，软目标是输入属于某类的概率以用softmax函数来估计：
$$
p(z_i,T)=\frac{exp(z_i/T)}{\Sigma_i exp(Z_J/T)}
$$
​	Hinton 2015 解释：软标签中含有大量的来源于教师模型暗知识（隐藏知识），基于暗知识理论，Response-Based 很容易理解，然而，Response-Based knowledge依赖于神经网络的最后一层，因此不能解决中间层的监督问题，中间层对于神经网络的表示学习十分重要。**由于软标签实际上是类概率分布，因此基于响应的知识精馏也局限于有监督学习。**

#### **Feature-Based Knowledge**

​	深度神经网络善于学习到不同层级的表征，因此中间层和输出层的都可以被用作知识来训练学生模型，中间层的Feature-Based Knowledge对于 Response-Based Knowledge是一个很好的补充，其主要思想是将教师和学生的特征激活直接匹配起来。一般情况下，Feature-Based Knowledge知识转移的蒸馏损失可表示为：
$$
L_{FeaD}=(f_t(x),f_s(x))=L_F(\Phi_t(f_t(x)),\Phi_s(f_s(x)))
$$
​	虽然基于特征的知识迁移为学生模型的学习提供了良好的信息，但如何有效地从教师模型中选择提示层，从学生模型中选择引导层，仍有待进一步研究。由于提示层和引导层的大小存在显著差异，如何正确匹配教师和学生的特征表示也需要探讨。

![image.png](https://s2.loli.net/2024/08/06/TH2KbBEu6yDecNj.png)

#### **Relation-Based Knowledge**

​	基于关系的知识和基于特征的知识都使用了教师模型中特定层次的输出。基于关系的知识进一步探索了不同层或数据样本之间的关系。一般情况下，基于特征图关系的关系知识的蒸馏损失可以表示为：
$$
L_{RelD}(f_t,f_s)=L_{R^1}(\varPsi_t(\bar{f_t},\hat{f_t}),\varPsi_s(\bar{f_s},\hat{f_s}))
$$
​	传统的知识转移方法往往涉及到个体知识的提炼。教师的个人软标签被直接提炼成学生中，实际上，经过提炼的知识不仅包含了特征信息，还包含了数据样本之间的相互关系。

![img](https://pic3.zhimg.com/80/v2-4924746952804a41b2f5e08121169c22_1440w.webp)